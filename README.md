<div align="center">

# ğŸ§  AURA Research Architect

### *Self-Evolving Cognitive Pipeline for Autonomous Research Synthesis*

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![DSPy](https://img.shields.io/badge/DSPy-Powered-6366f1?style=for-the-badge)](https://github.com/stanfordnlp/dspy)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

<br>

> âš ï¸ **EARLY PREVIEW** â€” This project is under active development. Features may change.

<br>

[ğŸ“š Documentation](#-architecture) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ¯ Features](#-features) â€¢ [ğŸ”§ Configuration](#-configuration)

</div>

---

## ğŸŒŸ What is AURA?

**AURA** (Autonomous Universal Research Architect) is a next-generation research synthesis system built on Stanford's [DSPy](https://github.com/stanfordnlp/dspy) framework. It transforms simple research questions into structured, grounded insights through a multi-stage cognitive pipeline.

Unlike traditional RAG systems, AURA features:

- ğŸ”„ **Self-Evolving Prompts** â€” Automatically optimizes its own prompts using DSPy's compilation
- ğŸ§  **Multiple Reasoning Modes** â€” Standard RAG, Multi-Hop, ReAct Agents, and Self-Reflection
- ğŸ’¸ **Hybrid LLM Engine** â€” Use free local models (Ollama) or cloud APIs (DeepSeek, OpenAI)
- ğŸ“Š **Built-in Evaluation** â€” LLM-as-judge metrics for quality assessment

---

## ğŸ¯ Features

### ğŸ—ï¸ Four Cognitive Architectures

| Mode | Description | Best For |
|------|-------------|----------|
| **Standard RAG** | Query â†’ Retrieve â†’ Synthesize | Quick factual research |
| **Multi-Hop Reasoning** | Iterative retrieval with context chaining | Complex multi-part questions |
| **ReAct Agent** | Autonomous tool use with reasoning traces | Dynamic problem solving |
| **Self-Reflecting Architect** | Generates multiple candidates, selects best | High-stakes synthesis |

### ğŸ”Œ Hybrid LLM Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AURA Hybrid Engine                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸ¦™ Ollama     â”‚   ğŸŒŠ DeepSeek   â”‚    ğŸ¤– OpenAI       â”‚
â”‚   (Free/Local)  â”‚   (Low Cost)    â”‚    (Premium)       â”‚
â”‚   llama3, phi   â”‚   deepseek-chat â”‚    gpt-4o-mini     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Ollama**: Run locally, no API key needed, completely free
- **DeepSeek**: Affordable cloud API with excellent performance
- **OpenAI**: Industry-standard, highest quality outputs

### ğŸ“š Flexible Retrieval

- **Local Knowledge Base**: Built-in mock retriever with curated AI/ML passages
- **ColBERTv2**: Wikipedia knowledge via Stanford's public endpoint
- **Extensible**: Easy to add custom retrievers

---

## ğŸ“ Project Structure

```
AURA/
â”œâ”€â”€ ğŸ“„ app.py                    # Streamlit Web Interface
â”œâ”€â”€ ğŸ“„ main.py                   # CLI Entry Point
â”œâ”€â”€ ğŸ“„ config.py                 # Configuration Constants
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ signatures/           # DSPy Signature Definitions
â”‚   â”‚   â”œâ”€â”€ search.py            # Query generation signature
â”‚   â”‚   â””â”€â”€ synthesis.py         # Research synthesis signature
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ modules/              # DSPy Module Implementations
â”‚   â”‚   â”œâ”€â”€ rag.py               # Standard RAG (AuraArchitect)
â”‚   â”‚   â”œâ”€â”€ multihop.py          # Multi-Hop Reasoning
â”‚   â”‚   â”œâ”€â”€ agent.py             # ReAct Agent
â”‚   â”‚   â””â”€â”€ reflector.py         # Self-Reflecting Module
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/
â”‚       â””â”€â”€ model_factory.py     # LLM Provider Factory
â”‚
â”œâ”€â”€ ğŸ“‚ pipelines/                # Optimization & Training
â”‚   â”œâ”€â”€ optimize_bootstrap.py    # BootstrapFewShot compilation
â”‚   â”œâ”€â”€ optimize_mipro.py        # MIPRO advanced optimization
â”‚   â””â”€â”€ distill.py               # Model distillation
â”‚
â”œâ”€â”€ ğŸ“‚ evaluation/               # Quality Assessment
â”‚   â”œâ”€â”€ data.py                  # Gold dataset definitions
â”‚   â””â”€â”€ metrics.py               # LLM-as-judge evaluators
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“„ .gitignore
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai) (for free local inference)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/AURA.git
cd AURA

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Pull a model (if using Ollama)
ollama pull llama3
# Or for faster inference:
ollama pull phi
```

### Run the Application

```bash
# Start the Streamlit UI
streamlit run app.py

# Or use the CLI
python main.py --help
```

Visit `http://localhost:8501` in your browser.

---

## ğŸ”§ Configuration

### Using Ollama (Free, Local)

1. Install [Ollama](https://ollama.ai)
2. Pull a model: `ollama pull phi` (fast) or `ollama pull llama3` (quality)
3. Select "Ollama (Free/Local)" in the sidebar
4. No API key needed!

### Using Cloud APIs

1. Select "DeepSeek" or "OpenAI" in the sidebar
2. Enter your API key
3. Choose your preferred model

### Environment Variables (Optional)

Create a `.env` file:

```env
OPENAI_API_KEY=sk-...
DEEPSEEK_API_KEY=sk-...
```

---

## ğŸ›ï¸ Architecture

### DSPy Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Research   â”‚â”€â”€â”€â”€â–¶â”‚    Query     â”‚â”€â”€â”€â”€â–¶â”‚  Retrieval   â”‚
â”‚     Goal     â”‚     â”‚  Generation  â”‚     â”‚   (k=3)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Insight    â”‚â—€â”€â”€â”€â”€â”‚   Passage    â”‚
                     â”‚  Synthesis   â”‚     â”‚   Context    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key DSPy Concepts Used

| Concept | Implementation | Purpose |
|---------|----------------|---------|
| **Signatures** | `GenerateSearchQuery`, `ResearchSynthesizer` | Define input/output contracts |
| **Modules** | `AuraArchitect`, `AuraMultiHop` | Encapsulate reasoning pipelines |
| **Predictors** | `dspy.Predict`, `dspy.ChainOfThought` | Execute LLM calls |
| **Retrievers** | `dspy.Retrieve`, `MockRetriever` | Fetch relevant context |

---

## ğŸ“Š Optimization & Evaluation

### Self-Evolving Prompts

AURA can automatically improve its prompts using DSPy's compilation:

```bash
# Bootstrap few-shot optimization
python pipelines/optimize_bootstrap.py

# Advanced MIPRO optimization
python pipelines/optimize_mipro.py
```

### Evaluation Metrics

The system includes LLM-as-judge evaluation:

```python
from evaluation.metrics import validate_aura_insight

# Assess quality of generated insights
score = validate_aura_insight(
    research_goal="...",
    generated_insight="...",
    gold_insight="..."
)
```

---

## ğŸ¨ UI Preview

The Streamlit interface features:

- ğŸŒ“ **Light/Dark Mode** â€” Automatically adapts to system theme
- ğŸ¯ **Clean, Minimal Design** â€” Focus on content, not decoration
- âš¡ **Real-time Progress** â€” See each pipeline step as it executes
- ğŸ“± **Responsive Layout** â€” Works on all screen sizes

---

## ğŸ—ºï¸ Roadmap

### Current Version (v0.1-preview)

- [x] Core DSPy architecture (Signatures, Modules)
- [x] Four cognitive modes (RAG, MultiHop, Agent, Reflector)
- [x] Hybrid LLM Engine (Ollama, DeepSeek, OpenAI)
- [x] Local MockRetriever fallback
- [x] Streamlit Web UI
- [x] Basic evaluation pipeline

### Upcoming Features

- [ ] Custom knowledge base upload (PDF, Markdown)
- [ ] Conversation memory & context persistence
- [ ] Advanced optimization with MIPRO v2
- [ ] API endpoint for programmatic access
- [ ] Export research reports (PDF, Markdown)
- [ ] Multi-language support

---

## ğŸ¤ Contributing

This project is in active development. Contributions are welcome!

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Stanford NLP DSPy](https://github.com/stanfordnlp/dspy) â€” The foundation of this project
- [Ollama](https://ollama.ai) â€” Making local LLMs accessible
- [Streamlit](https://streamlit.io) â€” Rapid UI development

---

<div align="center">

**Built with ğŸ’œ using DSPy**

*AURA â€” Transforming questions into structured knowledge*

</div>
#   A U R A - R e s e a r c h - A r c h i t e c t  
 